# ğŸŒ Practice Open-Source LLM

Minimalist workspace for local inference, quantization, and deployment of open-weights models.

![Python](https://img.shields.io/badge/python-3.10+-blue?style=flat-square)
![Ollama](https://img.shields.io/badge/Ollama-Inference-white?style=flat-square&logo=ollama)
![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97-Open%20Source-orange?style=flat-square)

---

## ğŸ“‚ Interactive Directory
*Click to navigate the repository:*

- [ğŸ“‚ **examples/**](./examples) â€” Practical scripts for model inference.
- [ğŸ“‚ **deployment/**](./deployment) â€” Local API and Docker setups.
- [ğŸ“‚ **benchmarks/**](./benchmarks) â€” Performance tracking (Tokens/sec).
- [ğŸ“‚ **setup/**](./setup) â€” Dependencies and environment configs.

---

## ğŸš€ Getting Started

```bash
# Clone and setup
git clone https://github.com/shreyaskr422/pratice-open-source-LLM.git
cd pratice-open-source-LLM
pip install -r requirements.txt

# Run default inference
python main.py --model llama3
